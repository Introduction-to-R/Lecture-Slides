---
title: "Lecture1d"
author: "Amaury de Vicq"
date: "5/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this lecture, we will occupy ourselves (i.) identifying outliers and (ii.) missing value imputation (NA). This is a crucial step in the data cleaning process and is often necessary for any kind of succesful data analysis. This is because (i.) missing data might prevent functions to work properly, and (ii.) outliers could corrupt and/or bias to your results.

# Part 1: Missing Data Handling

## Basic guidelines
  
Raw, unclean data often has many missing data (NAs). Especially in economic history, this is very common and could be due to a variety of different reasons. It might be that some of the sources you are using to create your dataset has several unreadable entries, that you cannot access the sources and/or that some years are missing. While it might be possible to simply overlook these missing observations or delete them for your dataset, this is not always the best approach.     

In fact it is important to distinguish between three categories of missing data:
  - MCAR (missing completely at random)
  - MAR (missing at random)
  - MNAR (missing not at random)

These categories determine how to best solve the missing data issues. Some basic guidelines include:
  - If the amount of NAs < 5% and they are MCAR, it generally acceptable to simply delete them and/or to use functions with a built-in NA handling feature
  - If the amount of NAs > 5% and they are MCAR, it is better to use simple imputation methods such as mean imputation and interpolation
  - If they are MAR or MNAR it is often advised to rely on more complex imputation methods such as MICE (multiple imputation by chained equations)
  
## Simple methods for missing data treatment

## Uncovering missing data

Let us import some historical data  using the `readxl` package, we introduced in a previous lecture. As we will soon find out, this datasets contains many missing values (NAs). We will then discuss some simple methods to address this issue.

First, let us install the `readxl` package.

```{r eval = FALSE}
install.packages('readxl')
```

And don't forget to 'turn it on':

```{r}
library(readxl)
```

Now, let us import the dataset using 'read_excel' command:

```{r}
NL <- read_excel("data/Netherlands_GDPperCapita_TerritorialRef_1946_2012_CCode_528(1).xlsx")
```


The 'NL' dataset provides historical information on GDP per Capita for the Netherlands (1800-2010). It is derived direcly from the Clio-infra website, with some minor alterations for the purpose of this lecture. Before we go any further, let us have a closer look at the actual dataset. By doing so, we might uncover some common issues that could arise in your own projects as well. 

Recommended first steps in the analysis of all sorts of datasets are:
- the 'str' function which tells us how the dataeset is structured.
- the 'summary' function which provides some summary statistics regarding the dataset.
- and finally the 'head' function which (if set as default) provides an overview of the first 5 rows of the dataset. But this is optional.

```{r}
str(NL)
summary(NL)
head(NL)
```


![](data/lecture1d_1.png)


Using these functions, gives us detailed and necessary information about this dataset. We find that it consists of 211 observations accross 5 different variables. 'Country Code', 'Country name' and 'Indicator' are somewhat redudant as we already know that this dataset contains information on 'GDP per Capita' for the Netherlands, so we could remove them. 

More imporantly however, R recognises the variable 'Data' as a character instead of number or an integer, so we have to change this.

Using the following code, we will change 'Data' into a numeric variable, and drop column 1:3 (albeit this is optional!). We will save the new dataset as NLclean, so we can easily revert back to the old dataset when we chose to do so. We will then once more use the 'summary' function on the newly created NLclean dataset.

```{r}
library(dplyr)
NLclean <- NL %>%
            mutate(Data = as.numeric(Data)) %>%
            select(4,5)

str(NLclean)

summary(NLclean)

head(NLclean)

```


Success! We now have much easier to interpret dataset which shows the GDP per Capita for the Netherlands on a yearly basis for the period 1800-2010. More importantly, it tells us that there are 14 missing values (NAs) in the 'Data' variable. In other words for 14 years out of the 211, we do not have any information regarding the GDP per Capita. Uncovering these NAs is a crucial step.


## Deleting data, relying on built-in NA handling features

As we briefly mentioned, we can opt to simply ignore these 14 NAs, or we can simply dete them using the 'na.omit' function

```{r}
NLomit <- na.omit(NLclean)

summary(NLomit)

```

As you can see, the dataset now consists of only 197 observations, and no longer has any NAs.

In additon many functions in R have a built-in NA handling feature designed specificially for this purpose. It is important to note that per default this function is often set to ignore the missing data. The function 'boxplot' for example has an argument 'na.action' which by default is set to NULL, indicating that it will ignore NAs.

If you feel confident you can ignore NA (i.e. because they are < 5% of all observations and MCAR) you can always check the vignette of any given function using '?' to see whether the function you wish to use has a built-in NA handling feature and what its default settings is set to ignore NAs. You can then proceed to use the function like you intended to.

Let's now look at a concrete example

```{r}
p1 <- boxplot(NLclean)

p1

```


While the boxplot might look rather strange, due to reasons which we will ignore for now, the function itself worked just fine. It ignored the missing values and plotted the boxplot regardless. So this is one, very basic way to cope with NAs in your dataset.

## Simple imputation methods dedicated to NA handling

Now, we introduce some simple imputation methods to deal with missing data. As long as the NAs are MCAR and below, or slighly above, 5% of all observations, these methods provide a reasonable solution.

There are several packages available which offer such a solution, but the most commonly used package is the `zoo` package. It contains many functions for missing data handling, such as:
- `na.locf()`, which is a generic function for replacing each NA with the most recent non-NA prior to it
- `na.spline()` & `na.approx()`, which are generic functions for replacing each NA with interpolated values
- `na.aggregate()`, which is a generic function for replacing each NA with aggregated values

As always you can find more information about these functions by reading their individual vignettes.

Let us start by installing the `zoo` package.

```{r eval = FALSE}
install.packages('zoo')
```

And don't forget to 'turn it on':

```{r}
library(zoo)
```

While all of these function have their advantages and disadvantages which you should carefully take into consideration when using them in your own analysis, a detailed discussion of all of these, will take us far beyond the introductory scope of this course. Therefore, we will focus our attention to the 'na.locf' function which is primary used for time series analysis.

Let us now take a closer look at what this function does.

```{r}
NLfull <- na.locf(NLclean)
summary(NLclean)
summary(NLfull)

```

![](data/lecture1d_4.png)

As you can see, when we compare the summary statistics of NLfull compared to NLclean, it no longer has any NAs. Clearly, the imputation function worked as it was intended to.

## Summary

Always check the presence of NAs using 'str' and 'summary'. 

Address NAs, by
- relying on built-in handling features: e.g. 'na.action' = NULL
- omitting / deleting NAs from the data: e.g. 'na.omit'
- simple imputationg methods: e.g. 'na.locf' for time series


# Part 2: Outliers

